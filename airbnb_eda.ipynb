{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "airbnb_eda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Eh8bo3ojqnTHmIGhDPJb4mBqntwQYIvk",
      "authorship_tag": "ABX9TyOQrqrmbdk4cjqLKXyho8kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nezihaksu/Airbnb_Istanbul/blob/main/airbnb_eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4jBsMmdw6_D"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "pd.set_option('display.max_columns', None)\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPxvywheZpr2"
      },
      "source": [
        "#Explatory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRMniZEBJkZv"
      },
      "source": [
        "DF = r'/content/drive/MyDrive/listings.csv'\r\n",
        "FILE_TYPE = \"csv\"\r\n",
        "IMPUTE = True\r\n",
        "ALLOWED_NAN_PERCENTAGE = 10\r\n",
        "DROP_KEYWORDS = [\"code\",\"zipcode\",\"link\",\"url\",\"id\",\"name\",\"thumbnail\",\"picture\",\"pic\",\"description\",\"note\"]\r\n",
        "\r\n",
        "class Eda():\r\n",
        "  \"\"\"Explore the dataset.\"\"\"\r\n",
        "  def __init__(self,df,file_type:str):\r\n",
        "    if file_type == \"xlsx\" or  file_type == \"xls\":\r\n",
        "      self.df = pd.read_excel(df,engine=\"python\")\r\n",
        "    self.df = pd.read_csv(df,engine=\"python\")\r\n",
        "    self.file_type = file_type\r\n",
        "\r\n",
        "  def __call__(self):\r\n",
        "    return self.df\r\n",
        "\r\n",
        "  def intro(self):\r\n",
        "    return \"===INFO===\",self.df.info(),\"===DESCRIPTION===\",self.df.describe(),\"===DTYPES==\",self.df.dtypes\r\n",
        "  \r\n",
        "  def unique_values(self):\r\n",
        "    #Unique values that are in features.\r\n",
        "    for column in self.df.columns:\r\n",
        "      print(column.upper()+ \" UNIQUE VALUES\")\r\n",
        "      print(str(df[column].unique())+\"\\n\")\r\n",
        "\r\n",
        "  def missing_values(self):\r\n",
        "\t  missing_percentage = self.df.isnull().sum()*100/len(self.df)\r\n",
        "\t  plt.figure(figsize=(5, 15))\r\n",
        "\t  missing_percentage.plot(kind='barh')\r\n",
        "\t  plt.xticks(rotation=90, fontsize=10)\r\n",
        "\t  plt.yticks(fontsize=5)\r\n",
        "\t  plt.xlabel(\"Missing Percentage\", fontsize=14)\r\n",
        "\t  plt.show()\r\n",
        "   \r\n",
        "  #Plotting histograms of the numerical features to see the distribution of each of them.\r\n",
        "  def dtype_histogram(self,data_type:str):\r\n",
        "    numerical_features = self.df.dtypes[self.df.dtypes == data_type].index.to_list()\r\n",
        "    self.df[numerical_features].hist(bins = 50,figsize = (20,15))\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzWsFcOFxXY"
      },
      "source": [
        "eda = Eda(df=DF,file_type=FILE_TYPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kFyZjIx20I3"
      },
      "source": [
        "class Preprocess():\r\n",
        "  \"\"\"Clean the dataset.\"\"\"\r\n",
        "  def __init__(self,df):\r\n",
        "    self.df = df\r\n",
        "\r\n",
        "  #Expanding one column dataframe into multiple columns according to split character.\r\n",
        "  def split_column_into_df(self,column_index:int,split_char:str):\r\n",
        "    if len(df.columns) == 1:\r\n",
        "      quotes_strip = list(self.df.columns)[0].replace(strip_char,'')\r\n",
        "      columns_split = quotes_strip.split(split_char)\r\n",
        "      self.df = self.df[self.df.iloc[:,0].name].str.split(pat = split_char,expand = True)\r\n",
        "      self.df.columns =  columns_split\r\n",
        "      self.df.replace(split_char,'',regex = True,inplace = True)\r\n",
        "    print(\"This method is only for explanding single column dataframes!\")\r\n",
        "    return self.df\r\n",
        "\r\n",
        "  def drop_missing_columns(self,percentage):\r\n",
        "    missing_percentage = self.df.isnull().sum()*100/len(self.df)\r\n",
        "    features_left = missing_percentage[missing_percentage < percentage].index\r\n",
        "    return self.df[features_left]\r\n",
        "\r\n",
        "  def drop_column_contains(self,keywords:list):\r\n",
        "    for keyword in keywords:\r\n",
        "      keyword_pattern = re.compile(keyword)\r\n",
        "      for column in self.df.columns:\r\n",
        "        if keyword_pattern.search(column):\r\n",
        "          self.df.drop(column,axis=1,inplace=True)\r\n",
        "    return self.df\r\n",
        "\r\n",
        "  #Loops over the values of the column until hitting the specified limit to find letter character to match and drop the column.\r\n",
        "  def _drop_value_type(self,pattern,search_limit:int):\r\n",
        "      for column in self.df.columns:\r\n",
        "        for value in self.df[column].values[:search_limit]:\r\n",
        "          if pattern.match(str(value)):\r\n",
        "            print(column)\r\n",
        "            self.df.drop(column,axis=1,inplace=True)\r\n",
        "\r\n",
        "      return self.df\r\n",
        "\r\n",
        "  def drop_text_columns(self,search_limit:int):\r\n",
        "    letter_pattern = re.compile(r'[A-z]')\r\n",
        "\r\n",
        "    return self._drop_value_type(letter_pattern,search_limit)\r\n",
        "    \r\n",
        "  def drop_date_columns(self,search_limit:int):\r\n",
        "    date_delimeters = [\"/\",\"-\",\".\",\" \"]\r\n",
        "    for delimeter in date_delimeters:\r\n",
        "      date_pattern = re.compile(r'([12]\\d{3}{}(0[1-9]|1[0-2]){}(0[1-9]|[12]\\d|3[01]))'.format(delimeter))\r\n",
        "\r\n",
        "      return _drop_value_type(date_pattern,search_limit)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi-3dRySDdWk"
      },
      "source": [
        "df = eda()\r\n",
        "preprocess = Preprocess(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}